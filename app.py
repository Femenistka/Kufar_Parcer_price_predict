"""
Streamlit UI –¥–ª—è –ø–∞—Ä—Å–µ—Ä–∞ Kufar.
"""

import streamlit as st
import sqlite3
from datetime import datetime
from typing import List
from scraper import KufarScraper, Database, ListingRaw
import time
from DB_functions import clear_db


# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(
    page_title="KeyScout - –ü–∞—Ä—Å–µ—Ä Kufar",
    page_icon="üéπ",
    layout="wide"
)


def scrape_all_pages(scraper: KufarScraper, region: str = "minsk", 
                     category: str = "klavishnye", **kwargs) -> List[ListingRaw]:
    """–ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü."""
    listings = []
    seen_ids = set()  # –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    current_cursor = None
    page = 1
    max_pages = 1000  # –ó–∞—â–∏—Ç–∞ –æ—Ç –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–≥–æ —Ü–∏–∫–ª–∞
    empty_pages_count = 0  # –°—á–µ—Ç—á–∏–∫ –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ–¥—Ä—è–¥
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    while page <= max_pages:
        status_text.text(f"–ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}...")
        progress_bar.progress(min(page / 100, 1.0))  # –ú–∞–∫—Å–∏–º—É–º 100 —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞
        
        url = scraper.build_search_url(region=region, category=category, 
                                      cursor=current_cursor, **kwargs)
        soup = scraper.fetch_page(url)
        
        if not soup:
            status_text.text(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}")
            empty_pages_count += 1
            if empty_pages_count >= 2:
                status_text.text(f"–î–≤–µ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ–¥—Ä—è–¥. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –ø–∞—Ä—Å–∏–Ω–≥–∞.")
                break
            page += 1
            continue
        
        cards = soup.find_all('a', {'data-testid': 'kufar-ad'})
        
        if not cards or len(cards) == 0:
            status_text.text(f"–û–±—ä—è–≤–ª–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page}")
            empty_pages_count += 1
            if empty_pages_count >= 2:
                status_text.text(f"–î–≤–µ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ–¥—Ä—è–¥. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –ø–∞—Ä—Å–∏–Ω–≥–∞.")
                break
            page += 1
            continue
        
        empty_pages_count = 0  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫, –µ—Å–ª–∏ –Ω–∞—à–ª–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏—è
        
        page_listings = []
        new_listings_count = 0
        
        for card in cards:
            listing = scraper.parse_listing_card(card)
            if listing and listing.source_id:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã
                if listing.source_id not in seen_ids:
                    seen_ids.add(listing.source_id)
                    page_listings.append(listing)
                    new_listings_count += 1
        
        # –ï—Å–ª–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –Ω–µ—Ç –Ω–æ–≤—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π, –≤–æ–∑–º–æ–∂–Ω–æ –º—ã –∑–∞—Ü–∏–∫–ª–∏–ª–∏—Å—å
        if new_listings_count == 0 and len(listings) > 0:
            status_text.text(f"–ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page} –Ω–µ—Ç –Ω–æ–≤—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π. –í–æ–∑–º–æ–∂–Ω–æ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç –∫–æ–Ω–µ—Ü.")
            break
        
        listings.extend(page_listings)
        status_text.text(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}: –Ω–∞–π–¥–µ–Ω–æ {len(page_listings)} –Ω–æ–≤—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π. –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö: {len(listings)}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Å–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
        next_cursor = scraper.extract_next_cursor(soup)
        if not next_cursor:
            status_text.text(f"–°–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ: {len(listings)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
            break
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø–æ–≤—Ç–æ—Ä—è–µ—Ç—Å—è –ª–∏ cursor (–∑–∞—â–∏—Ç–∞ –æ—Ç –∑–∞—Ü–∏–∫–ª–∏–≤–∞–Ω–∏—è)
        if current_cursor == next_cursor:
            status_text.text(f"Cursor –Ω–µ –∏–∑–º–µ–Ω–∏–ª—Å—è. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –ø–∞—Ä—Å–∏–Ω–≥–∞.")
            break
        
        current_cursor = next_cursor
        page += 1
        time.sleep(scraper.delay)
    
    progress_bar.progress(1.0)
    status_text.text(f"–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(listings)}")
    return listings


def format_price(price: float, currency: str) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ü–µ–Ω—ã –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è."""
    if price is None:
        return "–¶–µ–Ω–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞"
    return f"{price:,.0f} {currency}".replace(",", " ")


def format_date(date: datetime) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞—Ç—ã –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è."""
    if date is None:
        return "–î–∞—Ç–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞"
    return date.strftime("%d.%m.%Y %H:%M")


def display_listing_card(listing_data: dict):
    """–û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∫–∞—Ä—Ç–æ—á–∫–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏—è."""
    col1, col2 = st.columns([3, 1])
    
    with col1:
        st.markdown(f"### {listing_data['title'] or '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'}")
        if listing_data['description']:
            st.markdown(f"*{listing_data['description'][:200]}...*" if len(listing_data['description']) > 200 else f"*{listing_data['description']}*")
    
    with col2:
        st.markdown(f"**{format_price(listing_data['price'], listing_data['currency'])}**")
        st.markdown(f"üìç {listing_data['location'] or '–ù–µ —É–∫–∞–∑–∞–Ω–æ'}")
        st.markdown(f"üìÖ {format_date(listing_data['published_at'])}")
    
    if listing_data['url']:
        st.markdown(f"[üîó –û—Ç–∫—Ä—ã—Ç—å –æ–±—ä—è–≤–ª–µ–Ω–∏–µ]({listing_data['url']})")
    
    st.divider()


def get_listings_from_db(db_path: str = "keyscout.db") -> List[dict]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π –∏–∑ –ë–î."""
    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row
    cursor = conn.cursor()
    
    cursor.execute("""
        SELECT source_id, url, title, price, currency, published_at, 
               location, description, raw_text, created_at, updated_at
        FROM listings
        ORDER BY updated_at DESC
    """)
    
    rows = cursor.fetchall()
    conn.close()
    
    listings = []
    for row in rows:
        published_at = None
        if row['published_at']:
            try:
                published_at = datetime.fromisoformat(row['published_at'])
            except:
                pass
        
        listings.append({
            'source_id': row['source_id'],
            'url': row['url'],
            'title': row['title'],
            'price': row['price'],
            'currency': row['currency'] or 'BYN',
            'published_at': published_at,
            'location': row['location'],
            'description': row['description'] or row['raw_text'] or '',
            'raw_text': row['raw_text'],
            'created_at': row['created_at'],
            'updated_at': row['updated_at']
        })
    
    return listings


# –ì–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é - –≤–∫–ª–∞–¥–∫–∏ –≤ —à–∞–ø–∫–µ
tab1, tab2 = st.tabs(["‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞", "üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã"])

with tab1:
    st.title("üéπ KeyScout - –ü–∞—Ä—Å–µ—Ä –æ–±—ä—è–≤–ª–µ–Ω–∏–π Kufar")
    st.markdown("---")
    
    st.subheader("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞")
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–∞—Ä—Å–∏–Ω–≥–∞
    col1, col2 = st.columns(2)
    
    with col1:
        scrape_all = st.checkbox("–°–æ–±—Ä–∞—Ç—å –≤—Å–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è", value=False)
        # st.button("üßπ –û—á–∏—Å—Ç–∏—Ç—å –ë–î", type="primary", use_container_width=False, on_click=clear_db)
    
    with col2:
        
        if not scrape_all:
            num_pages = st.number_input("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü", min_value=1, max_value=100, value=1)
        else:
            num_pages = None
            st.info("–ë—É–¥—É—Ç —Å–æ–±—Ä–∞–Ω—ã –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è")
        st.button("üßπ –û—á–∏—Å—Ç–∏—Ç—å –ë–î", type="primary", use_container_width=False, on_click=clear_db)
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    with st.expander("–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã"):
        region = st.selectbox("–†–µ–≥–∏–æ–Ω", ["minsk", "gomel", "vitebsk", "grodno", "mogilev", "brest"], index=0)
        category = st.text_input("–ö–∞—Ç–µ–≥–æ—Ä–∏—è", value="klavishnye")
    
    # –ö–Ω–æ–ø–∫–∞ –∑–∞–ø—É—Å–∫–∞
    if st.button("üöÄ –ù–∞—á–∞—Ç—å –ø–∞—Ä—Å–∏–Ω–≥", type="primary", use_container_width=True):
        if not scrape_all and num_pages is None:
            st.error("–í—ã–±–µ—Ä–∏—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –∏–ª–∏ –≤–∫–ª—é—á–∏—Ç–µ –æ–ø—Ü–∏—é '–°–æ–±—Ä–∞—Ç—å –≤—Å–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è'")
        else:
            with st.spinner("–ü–∞—Ä—Å–∏–Ω–≥ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ..."):
                scraper = KufarScraper(delay=1.0, timeout=10)
                db = Database("keyscout.db")
                
                # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞
                test_params = {
                    'mkb': 'v.or:1,25',
                    'mki': 'v.or:1,5'
                }
                
                try:
                    if scrape_all:
                        listings = scrape_all_pages(
                            scraper, 
                            region=region, 
                            category=category, 
                            **test_params
                        )
                    else:
                        listings = scraper.scrape_search_results(
                            region=region,
                            category=category,
                            max_pages=num_pages,
                            **test_params
                        )
                    
                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
                    saved_count = db.save_listings(listings)
                    
                    st.success(f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω!")
                    st.info(f"üìä –ù–∞–π–¥–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(listings)}\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –ë–î: {saved_count}")
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ—Å–ª–µ–¥–Ω–µ–º –ø–∞—Ä—Å–∏–Ω–≥–µ –≤ session state
                    st.session_state['last_scrape_count'] = len(listings)
                    st.session_state['last_scrape_saved'] = saved_count
                    
                except Exception as e:
                    st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: {str(e)}")
                finally:
                    db.close()
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞, –µ—Å–ª–∏ –µ—Å—Ç—å
    if 'last_scrape_count' in st.session_state:
        st.markdown("---")
        st.subheader("–ü–æ—Å–ª–µ–¥–Ω–∏–π –ø–∞—Ä—Å–∏–Ω–≥")
        st.metric("–ù–∞–π–¥–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π", st.session_state['last_scrape_count'])
        st.metric("–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –ë–î", st.session_state['last_scrape_saved'])

with tab2:
    st.title("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞—Ä—Å–∏–Ω–≥–∞")
    st.markdown("---")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ë–î
    try:
        listings = get_listings_from_db()
        
        if not listings:
            st.info("üì≠ –û–±—ä—è–≤–ª–µ–Ω–∏–π –ø–æ–∫–∞ –Ω–µ—Ç. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –ø–∞—Ä—Å–∏–Ω–≥ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –Ω–∞—Å—Ç—Ä–æ–µ–∫.")
        else:
            st.success(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(listings)}")
            
            # –§–∏–ª—å—Ç—Ä—ã
            st.subheader("–§–∏–ª—å—Ç—Ä—ã")
            col1, col2, col3 = st.columns(3)
            
            with col1:
                filter_price_min = st.number_input("–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞", min_value=0, value=0)
            
            with col2:
                filter_price_max = st.number_input("–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞", min_value=0, value=0, 
                                                   help="0 = –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è")
            
            with col3:
                sort_by = st.selectbox("–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞", 
                                      ["–î–∞—Ç–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è (–Ω–æ–≤—ã–µ)", "–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏", "–¶–µ–Ω–∞ (–ø–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—é)", "–¶–µ–Ω–∞ (–ø–æ —É–±—ã–≤–∞–Ω–∏—é)"])
            
            # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤
            filtered_listings = listings.copy()
            
            if filter_price_min > 0:
                filtered_listings = [l for l in filtered_listings if l['price'] and l['price'] >= filter_price_min]
            
            if filter_price_max > 0:
                filtered_listings = [l for l in filtered_listings if l['price'] and l['price'] <= filter_price_max]
            
            # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞
            if sort_by == "–î–∞—Ç–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è (–Ω–æ–≤—ã–µ)":
                filtered_listings.sort(key=lambda x: x['updated_at'] or '', reverse=True)
            elif sort_by == "–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏":
                filtered_listings.sort(key=lambda x: x['published_at'] or datetime.min, reverse=True)
            elif sort_by == "–¶–µ–Ω–∞ (–ø–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—é)":
                filtered_listings.sort(key=lambda x: x['price'] or float('inf'))
            elif sort_by == "–¶–µ–Ω–∞ (–ø–æ —É–±—ã–≤–∞–Ω–∏—é)":
                filtered_listings.sort(key=lambda x: x['price'] or 0, reverse=True)
            
            st.markdown(f"**–ü–æ–∫–∞–∑–∞–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(filtered_listings)} –∏–∑ {len(listings)}**")
            st.markdown("---")
            
            # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∫–∞—Ä—Ç–æ—á–µ–∫
            for listing in filtered_listings:
                display_listing_card(listing)
                
    except Exception as e:
        st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö: {str(e)}")
        st.exception(e)

