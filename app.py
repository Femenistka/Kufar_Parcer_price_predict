"""
Streamlit UI –¥–ª—è –ø–∞—Ä—Å–µ—Ä–∞ Kufar.
"""

import streamlit as st
import sqlite3
from datetime import datetime
from typing import List
from scraper import KufarScraper, Database, ListingRaw
import time
import pandas as pd
import numpy as np
from DB_functions import clear_db


# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(
    page_title="KeyScout - –ü–∞—Ä—Å–µ—Ä Kufar",
    page_icon="üéπ",
    layout="wide"
)

def build_market_analytics_df(listings: list[dict]) -> pd.DataFrame:
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç listings (list[dict]) –≤ DataFrame –∏ —á–∏—Å—Ç–∏—Ç –±–∞–∑–æ–≤—ã–µ –ø–æ–ª—è."""
    df = pd.DataFrame(listings)

    # –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –ø–æ–ª—è
    for col in ["price", "market_price"]:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce")

    # –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫–∏, –≥–¥–µ –µ—Å—Ç—å –æ–±–µ —Ü–µ–Ω—ã
    df = df.dropna(subset=["price", "market_price"]).copy()

    # –≤—ã—á–∏—Å–ª–µ–Ω–∏—è
    df["delta"] = df["price"] - df["market_price"]               # >0 –¥–æ—Ä–æ–∂–µ —Ä—ã–Ω–∫–∞, <0 –¥–µ—à–µ–≤–ª–µ —Ä—ã–Ω–∫–∞
    df["gain"] = df["market_price"] - df["price"]                # –≤—ã–≥–æ–¥–∞ (–µ—Å–ª–∏ >0)
    df["gain_pct"] = (df["gain"] / df["market_price"]) * 100     # –≤—ã–≥–æ–¥–∞ –≤ %

    return df

def market_metrics(df: pd.DataFrame) -> dict:
    """–°—á–∏—Ç–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —Ä—ã–Ω–∫–∞."""
    if df.empty:
        return {}

    metrics = {
        "–û—Ü–µ–Ω–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π (–µ—Å—Ç—å price + market_price)": int(len(df)),
        "–°—Ä–µ–¥–Ω—è—è —Ä–∞–∑–Ω–∏—Ü–∞ price - market (Bias), BYN": float(df["delta"].mean()),
        "–ú–µ–¥–∏–∞–Ω–Ω–∞—è —Ä–∞–∑–Ω–∏—Ü–∞ price - market, BYN": float(df["delta"].median()),
        "–°—Ä–µ–¥–Ω–µ–µ |price - market| (MAE), BYN": float(df["delta"].abs().mean()),
        "–ú–µ–¥–∏–∞–Ω–∞ |price - market|, BYN": float(df["delta"].abs().median()),
        "–î–æ–ª—è –æ–±—ä—è–≤–ª–µ–Ω–∏–π –Ω–∏–∂–µ —Ä—ã–Ω–∫–∞ (price < market), %": float((df["price"] < df["market_price"]).mean() * 100),
        "–î–æ–ª—è –æ–±—ä—è–≤–ª–µ–Ω–∏–π –≤—ã—à–µ —Ä—ã–Ω–∫–∞ (price > market), %": float((df["price"] > df["market_price"]).mean() * 100),
        "–ú–∞–∫—Å –ø–µ—Ä–µ–ø–ª–∞—Ç–∞ (price - market), BYN": float(df["delta"].max()),
        "–ú–∞–∫—Å –≤—ã–≥–æ–¥–∞ (market - price), BYN": float(df["gain"].max()),
    }
    return metrics


def scrape_all_pages(scraper: KufarScraper, region: str = "minsk", 
                     category: str = "klavishnye", **kwargs) -> List[ListingRaw]:
    """–ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü."""
    listings = []
    seen_ids = set()  # –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    current_cursor = None
    page = 1
    max_pages = 1000  # –ó–∞—â–∏—Ç–∞ –æ—Ç –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–≥–æ —Ü–∏–∫–ª–∞
    empty_pages_count = 0  # –°—á–µ—Ç—á–∏–∫ –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ–¥—Ä—è–¥
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    while page <= max_pages:
        status_text.text(f"–ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}...")
        progress_bar.progress(min(page / 100, 1.0))  # –ú–∞–∫—Å–∏–º—É–º 100 —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞
        
        url = scraper.build_search_url(region=region, category=category, 
                                      cursor=current_cursor, **kwargs)
        soup = scraper.fetch_page(url)
        
        if not soup:
            status_text.text(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}")
            empty_pages_count += 1
            if empty_pages_count >= 2:
                status_text.text(f"–î–≤–µ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ–¥—Ä—è–¥. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –ø–∞—Ä—Å–∏–Ω–≥–∞.")
                break
            page += 1
            continue
        
        cards = soup.find_all('a', {'data-testid': 'kufar-ad'})
        
        if not cards or len(cards) == 0:
            status_text.text(f"–û–±—ä—è–≤–ª–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page}")
            empty_pages_count += 1
            if empty_pages_count >= 2:
                status_text.text(f"–î–≤–µ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ–¥—Ä—è–¥. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –ø–∞—Ä—Å–∏–Ω–≥–∞.")
                break
            page += 1
            continue
        
        empty_pages_count = 0  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫, –µ—Å–ª–∏ –Ω–∞—à–ª–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏—è
        
        page_listings = []
        new_listings_count = 0
        
        for card in cards:
            listing = scraper.parse_listing_card(card)
            if listing and listing.source_id:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã
                if listing.source_id not in seen_ids:
                    seen_ids.add(listing.source_id)
                    page_listings.append(listing)
                    new_listings_count += 1
        
        # –ï—Å–ª–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –Ω–µ—Ç –Ω–æ–≤—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π, –≤–æ–∑–º–æ–∂–Ω–æ –º—ã –∑–∞—Ü–∏–∫–ª–∏–ª–∏—Å—å
        if new_listings_count == 0 and len(listings) > 0:
            status_text.text(f"–ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page} –Ω–µ—Ç –Ω–æ–≤—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π. –í–æ–∑–º–æ–∂–Ω–æ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç –∫–æ–Ω–µ—Ü.")
            break
        
        listings.extend(page_listings)
        status_text.text(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}: –Ω–∞–π–¥–µ–Ω–æ {len(page_listings)} –Ω–æ–≤—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π. –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö: {len(listings)}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Å–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
        next_cursor = scraper.extract_next_cursor(soup)
        if not next_cursor:
            status_text.text(f"–°–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ: {len(listings)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
            break
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø–æ–≤—Ç–æ—Ä—è–µ—Ç—Å—è –ª–∏ cursor (–∑–∞—â–∏—Ç–∞ –æ—Ç –∑–∞—Ü–∏–∫–ª–∏–≤–∞–Ω–∏—è)
        if current_cursor == next_cursor:
            status_text.text(f"Cursor –Ω–µ –∏–∑–º–µ–Ω–∏–ª—Å—è. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –ø–∞—Ä—Å–∏–Ω–≥–∞.")
            break
        
        current_cursor = next_cursor
        page += 1
        time.sleep(scraper.delay)
    
    progress_bar.progress(1.0)
    status_text.text(f"–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(listings)}")
    return listings


def format_price(price: float, currency: str) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ü–µ–Ω—ã –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è."""
    if price is None:
        return "–¶–µ–Ω–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞"
    return f"{price:,.0f} {currency}".replace(",", " ")


def format_date(date: datetime) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞—Ç—ã –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è."""
    if date is None:
        return "–î–∞—Ç–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞"
    return date.strftime("%d.%m.%Y %H:%M")


def display_listing_card(listing_data: dict):
    """–û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∫–∞—Ä—Ç–æ—á–∫–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏—è."""
    col1, col2, col3 = st.columns([3, 2, 2])

    title = listing_data.get("title") or "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
    description = listing_data.get("description") or ""
    market_price = listing_data.get("market_price")
    price = listing_data.get("price")
    currency = listing_data.get("currency") or "BYN"
    location = listing_data.get("location") or "–ù–µ —É–∫–∞–∑–∞–Ω–æ"
    published_at = listing_data.get("published_at")
    url = listing_data.get("url")

    with col1:
        st.markdown(f"### {title}")
        if description:
            short = (description[:200] + "...") if len(description) > 200 else description
            st.markdown(f"*{short}*")

    with col2:
        if market_price is None:
            st.markdown("### üí© –Ω–µ —Ö–≤–∞—Ç–∏–ª–æ –¥–∞–Ω–Ω—ã—Ö")
        else:
            st.markdown(f"### {format_price(market_price, currency)}")

    with col3:
        if price is not None:
            st.markdown(f"**{format_price(price, currency)}**")
        else:
            st.markdown("**–¶–µ–Ω–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞**")

        st.markdown(f"üìç {location}")
        st.markdown(f"üìÖ {format_date(published_at)}")

    if url:
        st.markdown(f"[üîó –û—Ç–∫—Ä—ã—Ç—å –æ–±—ä—è–≤–ª–µ–Ω–∏–µ]({url})")

    st.divider()



def get_listings_from_db(db_path: str = "keyscout.db") -> List[dict]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π –∏–∑ –ë–î."""
    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row
    cursor = conn.cursor()
    
    cursor.execute("""
        SELECT source_id, url, title, price, currency, published_at, 
               location, description, raw_text, created_at, updated_at, market_price
        FROM listings_enriched
        ORDER BY updated_at DESC
    """)
    
    rows = cursor.fetchall()
    conn.close()
    
    listings = []
    for row in rows:
        published_at = None
        if row['published_at']:
            try:
                published_at = datetime.fromisoformat(row['published_at'])
            except:
                pass
        
        listings.append({
            'source_id': row['source_id'],
            'url': row['url'],
            'title': row['title'],
            'price': row['price'],
            'currency': row['currency'] or 'BYN',
            'published_at': published_at,
            'location': row['location'],
            'description': row['description'] or row['raw_text'] or '',
            'raw_text': row['raw_text'],
            'created_at': row['created_at'],
            'updated_at': row['updated_at'],
            'market_price': row['market_price'],
        })
    
    return listings


# –ì–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é - –≤–∫–ª–∞–¥–∫–∏ –≤ —à–∞–ø–∫–µ
tab1, tab2, tab3 = st.tabs(["–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞", "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã", "–ê–Ω–∞–ª–∏—Ç–∏–∫–∞"])

with tab1:
    st.title("üéπ KeyScout - –ü–∞—Ä—Å–µ—Ä –æ–±—ä—è–≤–ª–µ–Ω–∏–π Kufar")
    st.markdown("---")
    
    st.subheader("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞")
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–∞—Ä—Å–∏–Ω–≥–∞
    col1, col2 = st.columns(2)
    
    with col1:
        scrape_all = st.checkbox("–°–æ–±—Ä–∞—Ç—å –≤—Å–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è", value=True)
        # st.button("üßπ –û—á–∏—Å—Ç–∏—Ç—å –ë–î", type="primary", use_container_width=False, on_click=clear_db)
    
    with col2:
        
        if not scrape_all:
            num_pages = st.number_input("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü", min_value=1, max_value=100, value=1)
        else:
            num_pages = None
            st.info("–ë—É–¥—É—Ç —Å–æ–±—Ä–∞–Ω—ã –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è")
        st.button("üßπ –û—á–∏—Å—Ç–∏—Ç—å –ë–î", type="primary", use_container_width=False, on_click=clear_db)
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    with st.expander("–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã"):
        region = st.selectbox("–†–µ–≥–∏–æ–Ω", ["minsk", "gomel", "vitebsk", "grodno", "mogilev", "brest"], index=0)
        category = st.text_input("–ö–∞—Ç–µ–≥–æ—Ä–∏—è", value="klavishnye")
    
    # –ö–Ω–æ–ø–∫–∞ –∑–∞–ø—É—Å–∫–∞
    if st.button("üöÄ –ù–∞—á–∞—Ç—å –ø–∞—Ä—Å–∏–Ω–≥", type="primary", use_container_width=True):
        if not scrape_all and num_pages is None:
            st.error("–í—ã–±–µ—Ä–∏—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –∏–ª–∏ –≤–∫–ª—é—á–∏—Ç–µ –æ–ø—Ü–∏—é '–°–æ–±—Ä–∞—Ç—å –≤—Å–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è'")
        else:
            with st.spinner("–ü–∞—Ä—Å–∏–Ω–≥ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ..."):
                scraper = KufarScraper(delay=1.0, timeout=10)
                db = Database("keyscout.db")
                
                # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞
                test_params = {
                    'mkb': 'v.or:1,25',
                    'mki': 'v.or:1,5'
                }
                
                try:
                    if scrape_all:
                        listings = scrape_all_pages(
                            scraper, 
                            region=region, 
                            category=category, 
                            **test_params
                        )
                    else:
                        listings = scraper.scrape_search_results(
                            region=region,
                            category=category,
                            max_pages=num_pages,
                            **test_params
                        )
                    

                    db = Database("keyscout.db")
                    n = db.load_model_specs_csv("/Users/artemsaman/Desktop/KeyScout/–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏_–ø–æ_–º–æ–¥–µ–ª—è–º.csv")  # –ø—É—Ç—å —Å–≤–æ–π
                    print("–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏_–ø–æ_–º–æ–¥–µ–ª—è–º –∑–∞–≥—Ä—É–∂–µ–Ω—ã:", n)
                    # db.close()

                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
                    saved_count = db.save_listings(listings)
                    # 1) –ø–∞—Ä—Å–∏–Ω–≥ -> listings
                    saved_ids = db.save_listings_return_ids(listings)
                    saved_count = len(saved_ids)

                    # 2) –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è title -> Name/SubName/IndexModel
                    normalized_count = db.normalize_titles_for_ids(saved_ids)
                    
                    # 3) join -> listings_enriched
                    enriched_count = db.build_enriched_listings_table()
                    
                    # 4) predict + write back
                    stats = db.run_scoring_and_save_predictions(
                        model_path="models/SubName+OTHERS/price_model_market.joblib",
                        current_year=2026,
                        subname_min_count=3
                    )


                    st.success(f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω!")
                    st.info(
                        f"üìä –ù–∞–π–¥–µ–Ω–æ: {len(listings)}\n"
                        f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {len(saved_ids)}\n"
                        f"üßº –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ: {normalized_count}\n"
                        f"üîó –û–±–æ–≥–∞—â–µ–Ω–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏: {enriched_count}"
                    )

                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ—Å–ª–µ–¥–Ω–µ–º –ø–∞—Ä—Å–∏–Ω–≥–µ –≤ session state
                    st.session_state['last_scrape_count'] = len(listings)
                    st.session_state['last_scrape_saved'] = saved_count
                    st.session_state['last_normalized_count'] = normalized_count
                except Exception as e:
                    st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: {str(e)}")
                finally:
                    db.close()
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞, –µ—Å–ª–∏ –µ—Å—Ç—å
    if 'last_scrape_count' in st.session_state:
        st.markdown("---")
        st.subheader("–ü–æ—Å–ª–µ–¥–Ω–∏–π –ø–∞—Ä—Å–∏–Ω–≥")
        st.metric("–ù–∞–π–¥–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π", st.session_state['last_scrape_count'])
        st.metric("–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –ë–î", st.session_state['last_scrape_saved'])
        st.metric("–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ", st.session_state['last_normalized_count'])

with tab2:
    st.title("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞—Ä—Å–∏–Ω–≥–∞")
    st.markdown("---")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ë–î
    try:
        listings = get_listings_from_db()
        
        if not listings:
            st.info("üì≠ –û–±—ä—è–≤–ª–µ–Ω–∏–π –ø–æ–∫–∞ –Ω–µ—Ç. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –ø–∞—Ä—Å–∏–Ω–≥ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –Ω–∞—Å—Ç—Ä–æ–µ–∫.")
        else:
            st.success(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(listings)}")
            
            # –§–∏–ª—å—Ç—Ä—ã
            st.subheader("–§–∏–ª—å—Ç—Ä—ã")
            col1, col2, col3 = st.columns(3)
            
            with col1:
                filter_price_min = st.number_input("–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞", min_value=0, value=0)
            
            with col2:
                filter_price_max = st.number_input("–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞", min_value=0, value=0, 
                                                   help="0 = –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è")
            
            with col3:
                sort_by = st.selectbox("–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞", 
                                      ["–î–∞—Ç–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è (–Ω–æ–≤—ã–µ)", "–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏", "–¶–µ–Ω–∞ (–ø–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—é)", "–¶–µ–Ω–∞ (–ø–æ —É–±—ã–≤–∞–Ω–∏—é)"])
            
            # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤
            filtered_listings = listings.copy()
            
            if filter_price_min > 0:
                filtered_listings = [l for l in filtered_listings if l['price'] and l['price'] >= filter_price_min]
            
            if filter_price_max > 0:
                filtered_listings = [l for l in filtered_listings if l['price'] and l['price'] <= filter_price_max]
            
            # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞
            if sort_by == "–î–∞—Ç–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è (–Ω–æ–≤—ã–µ)":
                filtered_listings.sort(key=lambda x: x['updated_at'] or '', reverse=True)
            elif sort_by == "–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏":
                filtered_listings.sort(key=lambda x: x['published_at'] or datetime.min, reverse=True)
            elif sort_by == "–¶–µ–Ω–∞ (–ø–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—é)":
                filtered_listings.sort(key=lambda x: x['price'] or float('inf'))
            elif sort_by == "–¶–µ–Ω–∞ (–ø–æ —É–±—ã–≤–∞–Ω–∏—é)":
                filtered_listings.sort(key=lambda x: x['price'] or 0, reverse=True)
            
            st.markdown(f"**–ü–æ–∫–∞–∑–∞–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(filtered_listings)} –∏–∑ {len(listings)}**")
            st.markdown("---")
            
            # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∫–∞—Ä—Ç–æ—á–µ–∫
            for listing in filtered_listings:
                display_listing_card(listing)
                
    except Exception as e:
        st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö: {str(e)}")
        st.exception(e)


with tab3:
    st.subheader("üìä –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ —Ä—ã–Ω–∫–∞")

    # listings ‚Äî —ç—Ç–æ —Ç–æ, —á—Ç–æ —Ç—ã —É–∂–µ –ø–æ–ª—É—á–∞–µ—à—å —á–µ—Ä–µ–∑ get_listings_from_db()
    df_m = build_market_analytics_df(listings)

    if df_m.empty:
        st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏: –Ω—É–∂–Ω—ã –æ–±—ä—è–≤–ª–µ–Ω–∏—è, –≥–¥–µ –∑–∞–ø–æ–ª–Ω–µ–Ω—ã price –∏ market_price.")
    else:
        # 1) –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        m = market_metrics(df_m)

        # –∫—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ –º–µ—Ç—Ä–∏–∫ –≤ 3 –∫–æ–ª–æ–Ω–∫–∏
        c1, c2, c3 = st.columns(3)
        c1.metric("–û—Ü–µ–Ω–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π", m["–û—Ü–µ–Ω–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π (–µ—Å—Ç—å price + market_price)"])
        c2.metric("–î–æ–ª—è –Ω–∏–∂–µ —Ä—ã–Ω–∫–∞", f"{m['–î–æ–ª—è –æ–±—ä—è–≤–ª–µ–Ω–∏–π –Ω–∏–∂–µ —Ä—ã–Ω–∫–∞ (price < market), %']:.1f}%")
        c3.metric("–°—Ä–µ–¥–Ω—è—è |Œî| (MAE)", f"{m['–°—Ä–µ–¥–Ω–µ–µ |price - market| (MAE), BYN']:.0f} BYN")

        c1, c2, c3 = st.columns(3)
        c1.metric("Bias (price - market)", f"{m['–°—Ä–µ–¥–Ω—è—è —Ä–∞–∑–Ω–∏—Ü–∞ price - market (Bias), BYN']:.0f} BYN")
        c2.metric("–ú–∞–∫—Å –≤—ã–≥–æ–¥–∞", f"{m['–ú–∞–∫—Å –≤—ã–≥–æ–¥–∞ (market - price), BYN']:.0f} BYN")
        c3.metric("–ú–∞–∫—Å –ø–µ—Ä–µ–ø–ª–∞—Ç–∞", f"{m['–ú–∞–∫—Å –ø–µ—Ä–µ–ø–ª–∞—Ç–∞ (price - market), BYN']:.0f} BYN")

        with st.expander("–ü–æ–∫–∞–∑–∞—Ç—å –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏"):
            st.json({k: (round(v, 2) if isinstance(v, float) else v) for k, v in m.items()})

        st.divider()

        # 2) –§–∏–ª—å—Ç—Ä "–≤—ã–≥–æ–¥–Ω—ã—Ö" (price < market_price)
        st.subheader("üî• –í—ã–≥–æ–¥–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è (price < market_price)")

        # –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ—Ä–æ–≥–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, —É–¥–æ–±–Ω–æ)
        min_gain = st.slider("–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –≤—ã–≥–æ–¥–∞ (BYN)", 0, 500, 50, 10)
        min_gain_pct = st.slider("–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –≤—ã–≥–æ–¥–∞ (%)", 0, 50, 10, 1)

        df_bargains = df_m[(df_m["gain"] >= min_gain) & (df_m["gain_pct"] >= min_gain_pct)].copy()

        st.caption(f"–ù–∞–π–¥–µ–Ω–æ –≤—ã–≥–æ–¥–Ω—ã—Ö –ø–æ —Ñ–∏–ª—å—Ç—Ä–∞–º: {len(df_bargains)}")

        # —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞: —Å–∞–º—ã–µ –≤—ã–≥–æ–¥–Ω—ã–µ —Å–≤–µ—Ä—Ö—É
        df_bargains = df_bargains.sort_values(["gain", "gain_pct"], ascending=[False, False])

        # –¥–µ–ª–∞–µ–º –æ–±—Ä–∞—Ç–Ω–æ —Å–ø–∏—Å–æ–∫ dict –¥–ª—è display_listing_card
        bargain_listings = df_bargains.to_dict(orient="records")

        # 3) –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∫–∞—Ä—Ç–æ—á–µ–∫ —Ç–æ–ª—å–∫–æ –≤—ã–≥–æ–¥–Ω—ã—Ö
        for listing in bargain_listings:
            display_listing_card(listing)

